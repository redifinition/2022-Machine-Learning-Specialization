{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSr5LwZbuH7b"
      },
      "source": [
        "# Let's practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "ZpczJ9CHuH7Z",
        "outputId": "ff78760e-c9b6-4bd5-ce2e-a4097e192d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'myst_nb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-004d8f4ceb3b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gymnasium'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install plotly'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmyst_nb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'myst_nb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install plotly\n",
        "from myst_nb import glue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPuuQ0y3uH7b"
      },
      "source": [
        "We will see a very simple grid world problem. In this environment the agent start always in the same position and needs to reach a cell in the world.\n",
        "To complicate things for the agent, one cell is a trap and if the agent go on it the game is lost.\n",
        "\n",
        "Visually the world will look like this:\n",
        "\n",
        "```{figure} /lectures/mdp/dynamic-programming/grid_world.png\n",
        ":align: center\n",
        ":width: 70%\n",
        "```\n",
        "\n",
        "Since MDPs (Markov Decision Processes) are designed to solve stochastic problems, we will introduce some stochasticity to the problem. Specifically, we will add uncertainty to the agent's movements. When the agent decides to move in one direction, such as UP, there is an 80% chance of moving in the desired direction and a 10% chance of moving to one of the adjacent cells.\n",
        "\n",
        "```{figure} /lectures/mdp/dynamic-programming/transition.png\n",
        ":align: center\n",
        ":width: 50%\n",
        "```\n",
        "\n",
        "If the agent moves into a wall, the agent stays in the same cell instead.\n",
        "\n",
        "## Markov Decision Process\n",
        "\n",
        "Before implementing the environment and solving it, we need to define it as a MDP $\\Omega = (S,A,T,R)$, where:\n",
        "- $S$ is the state space: each state $s$ will be a cell of the grid such as $\\forall s \\in S, s \\in [0, 11]$.\n",
        "- $A$ is the action space: each action $a$ will be a direction such as $\\forall a \\in A, a \\in \\{UP, DOWN, LEFT, RIGHT\\}$.\n",
        "- $T$ is the transition function. Moving in a specific direction have a success chance of $80\\%$, and there is a $20\\%$ probability of going in the adjacent cells instead.\n",
        "- $R$ is the reward function: Entering the goal gives $+10$, falling in the trap gives $-10$, and all the other actions give $0$.\n",
        "\n",
        "## Environment Implementation\n",
        "\n",
        "Again, we will implement the environment using `Gym` conventions and call it `GridWorld`.\n",
        "\n",
        "```python\n",
        "class GridWorld(gymnasium.Env):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, action: int):\n",
        "    pass\n",
        "```\n",
        "\n",
        "### State space and Action space\n",
        "\n",
        "Based on our definition of the MDP, the state space represents all the states possible, which in this problem is all the cells the agent can reach, and the action space is all the action the agent can perform.\n",
        "\n",
        "In both cases the spaces are discrete, so we can use the `Discrete` type of `Gym` to represent them.\n",
        "\n",
        "```python\n",
        "self.action_space = spaces.Discrete(4) # Up, Down, Left, Right\n",
        "self.observation_space = spaces.Discrete(12) # 12 cells\n",
        "```\n",
        "\n",
        "```{margin} State space\n",
        "There is not one way to represent the state space for a problem.\n",
        "In this problem we could represent it by the grid itself where the agent will be represented by the cell having boolean equal to `True`.\n",
        "You would have `spaces.MultiDiscrete(np.array([3, 4]))`.\n",
        "```\n",
        "\n",
        "```{note}\n",
        "We consider a state space containing 12 states, so it includes the wall cell. The agent cannot technically go on it, but it eases the implementation so we include it.\n",
        "```\n",
        "\n",
        "### Transition function\n",
        "\n",
        "In reinforcement learning, we don't need to explicitly implement the transition function. However, solving MDPs with value iteration requires it:\n",
        "\n",
        "```{math}\n",
        "V(s_{k+1}) = \\sum_a\\pi(a|s)\\sum_{s'}p(s'|s,a)\\left[ r(s,a) + \\gamma v_k(s') \\right]\n",
        "```\n",
        "\n",
        "`Gym` doesn't provide any method or convention of doing it, so we'll create a variable `P` representing our transition function. It will be a dictionary of dictionaries, where the first key is the current state and the second key will be the action. It will return the list of list containing the probability, the state and the reward.\n",
        "\n",
        "```python\n",
        "self.P = { 0: { 0: [[0.9, 0, 0], [0.1, 1, 0]], 1: [[0.8, 4, 0], [0.1, 1, 0], [0.1, 0, 0]], 2: [[0.9, 0, 0], [0.1, 4, 0]], 3: [[0.8, 1, 0], [0.1, 4, 0], [0.1, 0, 0]]},\n",
        "               1: { 0: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]],  1: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]], 2: [[0.8, 0, 0], [0.2, 1, 0]], 3: [[0.8, 2, 0], [0.2, 1, 0]]},\n",
        "               2: { 0: [[0.8, 2, 0], [0.1, 1, 0], [0.1, 3, 0]], 1: [[0.8, 6, 0], [0.1, 1, 0], [0.1, 3, 0]], 2: [[0.8, 1, 0], [0.1, 2, 0], [0.1, 6, 0]], 3: [[0.8, 3, 0], [0.1, 2, 0], [0.1, 6, 0]]},\n",
        "               3: { 0: [[0.9, 3, 0], [0.1, 2, 0]], 1: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 3, 0]], 2: [[0.8, 2, 0], [0.1, 3, 0], [0.1, 7, -10]], 3: [[0.9, 3, 0], [0.1, 7, -10]]},\n",
        "               4: { 0: [[0.8, 0, 0], [0.2, 4, 0]], 1: [[0.8, 8, 0], [0.2, 4, 0]], 2: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]], 3: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]]},\n",
        "               5: { 0: [[1, 5, 0]], 1: [[1, 5, 0]], 2: [[1, 5, 0]], 3: [[1, 5, 0]]},\n",
        "               6: { 0: [[0.8, 2, 0], [0.1, 6, 0], [0.1, 7, -10]], 1: [[0.8, 10, 0], [0.1, 6, 0], [0.1, 7, -10]], 2: [[0.8, 6, 0], [0.1, 10, 0], [0.1, 2, 0]], 3: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 10, 0]]},\n",
        "               7: { 0: [[1, 7, -10]], 1: [[1, 7, -10]], 2: [[1, 7, -10]], 3: [[1, 7, -10]]},\n",
        "               8: { 0: [[0.8, 4, 0], [0.1, 9, 0], [0.1, 8, 0]], 1: [[0.9, 8, 0], [0.1, 9, 0]], 2: [[0.9, 8, 0], [0.1, 4, 0]], 3: [[0.8, 9, 0], [0.1, 4, 0], [0.1, 8, 0]]},\n",
        "               9: { 0: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 1: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 2: [[0.8, 8, 0], [0.2, 9, 0]], 3: [[0.8, 10, 0], [0.2, 9, 0]]},\n",
        "               10: { 0: [[0.8, 6, 0], [0.1, 9, 0], [0.1, 11, 10]], 1: [[0.8, 10, 0], [0.1, 9, 0], [0.1, 11, 10]], 2: [[0.8, 9, 0], [0.1, 6, 0], [0.1, 10, 0]], 3: [[0.8, 11, 10], [0.1, 6, 0], [0.1, 10, 0]]},\n",
        "               11: { 0: [[1, 11, 10]], 1: [[1, 11, 10]], 2: [[1, 11, 10]], 3: [[1, 11, 10]]},\n",
        "              }\n",
        "```\n",
        "\n",
        "It is preferable to not use the variable `P` for applying the transition in the environment. We can implement it by modifying the action based on a random number to simulate the error. If the random number is below $0.8$, then it is a success and the action stays the same. Otherwise the action is changed to the clockwise or counterclockwise action.\n",
        "\n",
        "```{figure} /lectures/mdp/dynamic-programming/actions.png\n",
        ":align: center\n",
        ":width: 50%\n",
        "```\n",
        "\n",
        "```python\n",
        "prob = np.random.random()\n",
        "if prob < 0.80:\n",
        "  actual_action = action\n",
        "elif prob < 0.90:\n",
        "  # Adjacent cell \"clockwise\"\n",
        "  actual_action = (action + 1) % 4\n",
        "else:\n",
        "  # Adjacent cell \"counterclockwise\"\n",
        "  actual_action = (action - 1) % 4  \n",
        "```\n",
        "\n",
        "Our state is represented as an integer, but we can convert it to coordinates.\n",
        "\n",
        "```python\n",
        "r = np.floor(self.state / 3)\n",
        "c = self.state % 4\n",
        "```\n",
        "\n",
        "Once it is done we can apply the action and modify the coordinates.\n",
        "\n",
        "```python\n",
        "if actual_action == 0:\n",
        "  r = max(0, r - 1)\n",
        "elif actual_action == 2:\n",
        "  r = min(2, r + 1)\n",
        "elif actual_action == 1:\n",
        "  c = max(0, c - 1)\n",
        "elif actual_action == 3:\n",
        "  c = min(3, c + 1)\n",
        "```\n",
        " And finally, we convert it back to an integer.\n",
        "\n",
        "```python\n",
        "self.state = r * 4 + c\n",
        "```\n",
        "\n",
        "If we put everything together, we obtain a function that will manage the transition for us.\n",
        "\n",
        "```python\n",
        "\n",
        "  def _transition(self, action: int):\n",
        "    \"\"\"\n",
        "    Transition function.\n",
        "    :param action: Action to take\n",
        "    \"\"\"\n",
        "    r = np.floor(self.state / 3)\n",
        "    c = self.state % 4\n",
        "\n",
        "    prob = np.random.random()\n",
        "    if prob < 0.80:\n",
        "      actual_action = action\n",
        "    elif prob < 0.90:\n",
        "      # Adjacent cell \"clockwise\"\n",
        "      actual_action = (action + 1) % 4\n",
        "    else:\n",
        "      # Adjacent cell \"counterclockwise\"\n",
        "      actual_action = (action - 1) % 4\n",
        "\n",
        "\n",
        "    if actual_action == 0:\n",
        "      r = max(0, r - 1)\n",
        "    elif actual_action == 2:\n",
        "      r = min(2, r + 1)\n",
        "    elif actual_action == 1:\n",
        "      c = max(0, c - 1)\n",
        "    elif actual_action == 3:\n",
        "      c = min(3, c + 1)\n",
        "    self.state = r * 4 + c\n",
        "```\n",
        "\n",
        "### Reward function\n",
        "\n",
        "The reward is straightforward, if we reach the $+10$ ($-10$) cell the agent receive $+10$ ($-10$), otherwise the rewards is always $0$.\n",
        "\n",
        "We implement the reward directly in the `step` function.\n",
        "\n",
        "```python\n",
        "\n",
        "  def step(self, action: int):\n",
        "    self._transition(action)\n",
        "    \n",
        "    done = False\n",
        "\n",
        "    reward = 0\n",
        "    if self.state == 11:\n",
        "      reward = 10\n",
        "      done = True\n",
        "    elif self.state == 7:\n",
        "      reward = -10\n",
        "      done = True\n",
        "\n",
        "    # Return the observation, reward, done flag, and info\n",
        "    return self.state, reward, done, {}\n",
        "```\n",
        "\n",
        "### Everything together\n",
        "\n",
        "The full environment is provided below. You will see it contains a way to render the environment, it can be useful sometimes.\n",
        "\n",
        "```{admonition} Activity\n",
        ":class: activity\n",
        "\n",
        "The function `render` doesn't show the position of the agent and it is up to you to finish it.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "PH_6v79EuH7c"
      },
      "outputs": [],
      "source": [
        "import gymnasium\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "class GridWorld(gymnasium.Env):\n",
        "  def __init__(self):\n",
        "    # Define the action and observation spaces\n",
        "    self.action_space = spaces.Discrete(4) # Up, Down, Left, Right\n",
        "    self.observation_space = spaces.Discrete(12) # 12 cells\n",
        "\n",
        "    self.P = { 0: { 0: [[0.9, 0, 0], [0.1, 1, 0]], 1: [[0.8, 4, 0], [0.1, 1, 0], [0.1, 0, 0]], 2: [[0.9, 0, 0], [0.1, 4, 0]], 3: [[0.8, 1, 0], [0.1, 4, 0], [0.1, 0, 0]]},\n",
        "               1: { 0: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]],  1: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]], 2: [[0.8, 0, 0], [0.2, 1, 0]], 3: [[0.8, 2, 0], [0.2, 1, 0]]},\n",
        "               2: { 0: [[0.8, 2, 0], [0.1, 1, 0], [0.1, 3, 0]], 1: [[0.8, 6, 0], [0.1, 1, 0], [0.1, 3, 0]], 2: [[0.8, 1, 0], [0.1, 2, 0], [0.1, 6, 0]], 3: [[0.8, 3, 0], [0.1, 2, 0], [0.1, 6, 0]]},\n",
        "               3: { 0: [[0.9, 3, 0], [0.1, 2, 0]], 1: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 3, 0]], 2: [[0.8, 2, 0], [0.1, 3, 0], [0.1, 7, -10]], 3: [[0.9, 3, 0], [0.1, 7, -10]]},\n",
        "               4: { 0: [[0.8, 0, 0], [0.2, 4, 0]], 1: [[0.8, 8, 0], [0.2, 4, 0]], 2: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]], 3: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]]},\n",
        "               5: { 0: [[1, 5, 0]], 1: [[1, 5, 0]], 2: [[1, 5, 0]], 3: [[1, 5, 0]]},\n",
        "               6: { 0: [[0.8, 2, 0], [0.1, 6, 0], [0.1, 7, -10]], 1: [[0.8, 10, 0], [0.1, 6, 0], [0.1, 7, -10]], 2: [[0.8, 6, 0], [0.1, 10, 0], [0.1, 2, 0]], 3: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 10, 0]]},\n",
        "               7: { 0: [[1, 7, -10]], 1: [[1, 7, -10]], 2: [[1, 7, -10]], 3: [[1, 7, -10]]},\n",
        "               8: { 0: [[0.8, 4, 0], [0.1, 9, 0], [0.1, 8, 0]], 1: [[0.9, 8, 0], [0.1, 9, 0]], 2: [[0.9, 8, 0], [0.1, 4, 0]], 3: [[0.8, 9, 0], [0.1, 4, 0], [0.1, 8, 0]]},\n",
        "               9: { 0: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 1: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 2: [[0.8, 8, 0], [0.2, 9, 0]], 3: [[0.8, 10, 0], [0.2, 9, 0]]},\n",
        "               10: { 0: [[0.8, 6, 0], [0.1, 9, 0], [0.1, 11, 10]], 1: [[0.8, 10, 0], [0.1, 9, 0], [0.1, 11, 10]], 2: [[0.8, 9, 0], [0.1, 6, 0], [0.1, 10, 0]], 3: [[0.8, 11, 10], [0.1, 6, 0], [0.1, 10, 0]]},\n",
        "               11: { 0: [[1, 11, 10]], 1: [[1, 11, 10]], 2: [[1, 11, 10]], 3: [[1, 11, 10]]},\n",
        "              }\n",
        "\n",
        "    # Initialize the state\n",
        "    self.state = 0\n",
        "\n",
        "  def step(self, action: int):\n",
        "\n",
        "    self._transition(action)\n",
        "\n",
        "    done = False\n",
        "\n",
        "    reward = 0\n",
        "    if self.state == 11:\n",
        "      reward = 10\n",
        "      done = True\n",
        "    elif self.state == 7:\n",
        "      reward = -10\n",
        "      done = True\n",
        "\n",
        "\n",
        "    # Return the observation, reward, done flag, and info\n",
        "    return self.state, reward, done, {}\n",
        "\n",
        "  def _transition(self, action: int):\n",
        "    \"\"\"\n",
        "    Transition function.\n",
        "    :param action: Action to take\n",
        "    \"\"\"\n",
        "    r = np.floor(self.state / 4)\n",
        "    c = self.state % 3\n",
        "\n",
        "    prob = np.random.random()\n",
        "    if prob < 0.80:\n",
        "      actual_action = action\n",
        "    elif prob < 0.90:\n",
        "      # Adjacent cell \"clockwise\"\n",
        "      actual_action = (action + 1) % 4\n",
        "    else:\n",
        "      # Adjacent cell \"counter clockwise\"\n",
        "      actual_action = (action - 1) % 4\n",
        "\n",
        "\n",
        "    if actual_action == 0:\n",
        "      r = max(0, r - 1)\n",
        "    elif actual_action == 2:\n",
        "      r = min(2, r + 1)\n",
        "    elif actual_action == 1:\n",
        "      c = max(0, c - 1)\n",
        "    elif actual_action == 3:\n",
        "      c = min(3, c + 1)\n",
        "    self.state = r * 4 + c\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment.\n",
        "    \"\"\"\n",
        "    self.state = 0\n",
        "    return self.state\n",
        "\n",
        "  def render(self, render=\"human\"):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(0, 4)\n",
        "    ax.set_ylim(0, 3)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "\n",
        "    for i in range(4):\n",
        "      for j in range(3):\n",
        "        if j * 4 + i == 11:\n",
        "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='green')\n",
        "          ax.add_patch(rect)\n",
        "        elif j * 4 + i == 7:\n",
        "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='red')\n",
        "          ax.add_patch(rect)\n",
        "        elif j * 4 + i == 5:\n",
        "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='grey')\n",
        "          ax.add_patch(rect)\n",
        "        else:\n",
        "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='white')\n",
        "          ax.add_patch(rect)\n",
        "\n",
        "    ax.tick_params(axis='both',       # changes apply to both axis\n",
        "                    which='both',      # both major and minor ticks are affected\n",
        "                    bottom=False,      # ticks along the bottom edge are off\n",
        "                    top=False,         # ticks along the top edge are off\n",
        "                    left=False,\n",
        "                    right=False,\n",
        "                    labelbottom=False,\n",
        "                    labelleft=False) # labels along the bottom edge are off\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_LETqMHuH7d"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "Value Iteration is very easy to implement, and take advantage of the information provide by the environment to simplify the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsaDxf_7uH7d"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma=0.9, theta=0.0001):\n",
        "  V = np.zeros(env.observation_space.n)\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(env.observation_space.n):\n",
        "      v = V[s]\n",
        "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "      delta = max(delta, np.abs(v - V[s]))\n",
        "      V[7] = -10\n",
        "      V[11] = 10\n",
        "    if delta < theta:\n",
        "      break\n",
        "  pi = np.zeros(env.observation_space.n)\n",
        "  for s in range(env.observation_space.n):\n",
        "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "  return V, pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKbo6QdSuH7d"
      },
      "source": [
        "We can use the observation space to intialize the value function with zeros.\n",
        "\n",
        "```python\n",
        "V = np.zeros(env.observation_space.n)\n",
        "```\n",
        "\n",
        "We update the value for each state by looping other them (using the observation space again) and we apply the formula. This is where we actually need `P`, because it simplifies the code a lot.\n",
        "\n",
        "```python\n",
        "V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "```\n",
        "```{margin} Theta\n",
        "The value of theta is up to you, but if theta is too big yo may not converge to the optimal policy.\n",
        "Too small, it could take a long time to converge.\n",
        "```\n",
        "\n",
        "Finally, once we converged (based on $\\theta$) we calculate the policy the same way we calculated the value function but we use `argmax` instead.\n",
        "\n",
        "```python\n",
        "for s in range(env.observation_space.n):\n",
        "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "```\n",
        "\n",
        "While we run the algorithm, we can observe the evolution of the value function for each state. Value Iteration being a dynamic programming algorithm, we see that the first state being updated is the one close to the goal. In the following steps, the states are being progressively updated until it converges.\n",
        "\n",
        "```{margin} Example\n",
        "In the example the values are rounded to the nearest integers for rendering purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "T1X4zCAouH7d"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, clear_output\n",
        "\n",
        "def value_iteration_interact(env, gamma=0.9, theta=0.0001, step=0):\n",
        "  V = np.zeros(env.observation_space.n)\n",
        "  for i in range(step):\n",
        "    delta = 0\n",
        "    for s in range(env.observation_space.n):\n",
        "      v = V[s]\n",
        "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "      delta = max(delta, np.abs(v - V[s]))\n",
        "      V[7] = -10\n",
        "      V[11] = 10\n",
        "    if delta < theta:\n",
        "      break\n",
        "  pi = np.zeros(env.observation_space.n)\n",
        "  for s in range(env.observation_space.n):\n",
        "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "  return V, pi\n",
        "\n",
        "def render(labels):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_xlim(0, 4)\n",
        "  ax.set_ylim(0, 3)\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "  for i in range(4):\n",
        "    for j in range(3):\n",
        "      if j * 4 + i == 11:\n",
        "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='green')\n",
        "        ax.add_patch(rect)\n",
        "      elif j * 4 + i == 7:\n",
        "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='red')\n",
        "        ax.add_patch(rect)\n",
        "      elif j * 4 + i == 5:\n",
        "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='grey')\n",
        "        ax.add_patch(rect)\n",
        "      else:\n",
        "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='white')\n",
        "        ax.add_patch(rect)\n",
        "      ax.text(i + 0.5, j + 0.5, int(labels[j * 4 + i]), ha='center', va='center')\n",
        "\n",
        "  ax.tick_params(axis='both',        # changes apply to both axis\n",
        "                  which='both',      # both major and minor ticks are affected\n",
        "                  bottom=False,      # ticks along the bottom edge are off\n",
        "                  top=False,         # ticks along the top edge are off\n",
        "                  left=False,\n",
        "                  right=False,\n",
        "                  labelbottom=False,\n",
        "                  labelleft=False)   # labels along the bottom edge are off\n",
        "  display(fig)\n",
        "  plt.close(fig)\n",
        "\n",
        "def update(gamma=0.9, step=0):\n",
        "  V, pi = value_iteration_interact(env, gamma, step=step)\n",
        "  render(V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "uNb35bqbuH7e"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import *\n",
        "\n",
        "env = GridWorld()\n",
        "interact(update, gamma = (0.5,0.9,0.1), step = (0, 50, 1));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-input"
        ],
        "id": "paBCvjgAuH7e"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "def value_iteration_interact(env, gamma=0.9, theta=0.0001, step=0):\n",
        "  V = np.zeros(env.observation_space.n)\n",
        "  V[7] = -10\n",
        "  V[11] = 10\n",
        "  for i in range(step):\n",
        "    delta = 0\n",
        "    for s in range(env.observation_space.n):\n",
        "      v = V[s]\n",
        "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "      delta = max(delta, np.abs(v - V[s]))\n",
        "      V[7] = -10\n",
        "      V[11] = 10\n",
        "    if delta < theta:\n",
        "      break\n",
        "  pi = np.zeros(env.observation_space.n)\n",
        "  for s in range(env.observation_space.n):\n",
        "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "  return V, pi\n",
        "\n",
        "\n",
        "env = GridWorld()\n",
        "# Create figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add traces, one for each slider step\n",
        "for step in range(0, 15):\n",
        "    V, _ = value_iteration_interact(env, 0.9, step=step)\n",
        "    x, y, text = [], [], []\n",
        "    for i in range(4):\n",
        "      for j in range(3):\n",
        "        if j * 4 + i == 11:\n",
        "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"green\", layer=\"below\")\n",
        "        elif j * 4 + i == 7:\n",
        "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"red\", layer=\"below\")\n",
        "        elif j * 4 + i == 5:\n",
        "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"gray\", layer=\"below\")\n",
        "        else:\n",
        "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"white\", layer=\"below\")\n",
        "        x.append(i + 0.5)\n",
        "        y.append(j + 0.5)\n",
        "        text.append(int(V[j * 4 + i]))\n",
        "    fig.add_trace(go.Scatter(\n",
        "      x=x,\n",
        "      y=y,\n",
        "      text=text,\n",
        "      mode=\"text\",\n",
        "      textfont=dict(\n",
        "          color=\"black\",\n",
        "          size=18,\n",
        "          family=\"Arial\",\n",
        "      ),\n",
        "      visible=False\n",
        "    )\n",
        ")\n",
        "fig.data[0].visible = True\n",
        "# Create and add slider\n",
        "steps = []\n",
        "for i in range(len(fig.data)):\n",
        "    step = dict(\n",
        "        method=\"update\",\n",
        "        args=[{\"visible\": [False] * len(fig.data)}],  # layout attribute\n",
        "    )\n",
        "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
        "    steps.append(step)\n",
        "\n",
        "sliders = [dict(\n",
        "    active=0,\n",
        "    pad={\"t\": 50},\n",
        "    steps=steps\n",
        ")]\n",
        "\n",
        "fig.update_xaxes(\n",
        "    showticklabels=False,\n",
        "    showgrid=False,\n",
        "    zeroline=False,\n",
        "    automargin=True\n",
        ")\n",
        "\n",
        "fig.update_yaxes(\n",
        "    showticklabels=False,\n",
        "    showgrid=False,\n",
        "    zeroline=False,\n",
        "    scaleanchor=\"x\",\n",
        "    scaleratio=1,\n",
        "    automargin=True\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    sliders=sliders\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTrGMq23uH7e"
      },
      "source": [
        "It is also possible to study how the value function convergences by plotting the value of $\\delta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "UCwTcDmeuH7e"
      },
      "outputs": [],
      "source": [
        "def value_iteration_conv(env, gamma=0.9, theta=0.0001):\n",
        "  V = np.zeros(env.observation_space.n)\n",
        "  diff = []\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(env.observation_space.n):\n",
        "      v = V[s]\n",
        "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "      delta = max(delta, np.abs(v - V[s]))\n",
        "    diff.append(delta)\n",
        "    if delta < theta:\n",
        "      break\n",
        "  pi = np.zeros(env.observation_space.n)\n",
        "  for s in range(env.observation_space.n):\n",
        "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
        "  return V, pi, diff\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efRosVJeuH7e"
      },
      "source": [
        "````{margin} Impact of the discount rate\n",
        "If we are setting the discount rate to $\\gamma=0.5$ the algorithms converges quickly. However, it doesn't mean it converges to the same policy or that the policy will perform similarly to the previous one.\n",
        "\n",
        "```{glue:} vi_margin\n",
        "```\n",
        "````"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTC4LST1uH7e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_vi_conv(deltas):\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    x = np.linspace(0, len(deltas), len(deltas))\n",
        "\n",
        "    fig, ax1= plt.subplots()\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    ax1.plot(x, deltas, linewidth=2.0, color=\"C1\")\n",
        "    ax1.set_title(\"Evolution of the value of Delta\")\n",
        "    ax1.set_ylabel(\"Delta\")\n",
        "    ax1.set_xlabel(\"Episodes\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-input"
        ],
        "id": "KTolIzYWuH7e"
      },
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "V, pi, diff = value_iteration_conv(env)\n",
        "plot_vi_conv(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "7SfIZ9diuH7e"
      },
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "V, pi, diff = value_iteration_conv(env, gamma=0.5)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "x = np.linspace(0, len(diff), len(diff))\n",
        "\n",
        "fig, ax1= plt.subplots()\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "ax1.plot(x, diff, linewidth=2.0, color=\"C1\")\n",
        "ax1.set_title(\"Evolution of the value of Delta\")\n",
        "ax1.set_ylabel(\"Delta\")\n",
        "ax1.set_xlabel(\"Episodes\")\n",
        "glue(\"vi_margin\", fig, display=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GtDzRpMuH7e"
      },
      "source": [
        "```{important}\n",
        "Changing theta will increase or decrease the time it will take for the algorithm to stop, but it will impact the quality of the policy.\n",
        "If the algorithms stop too soon, meaning before it converges, the policy obtained could be suboptimal. However, larger problems can be too large to be calculated optimally, and a partial solution may be enough.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}